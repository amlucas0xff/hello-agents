<div align="right">
  <a href="./Chapter4-Building-Classic-Agent-Paradigms.md">English</a> | <a href="./Á¨¨ÂõõÁ´†%20Êô∫ËÉΩ‰ΩìÁªèÂÖ∏ËåÉÂºèÊûÑÂª∫.md">‰∏≠Êñá</a> | Portugu√™s
</div>

# Cap√≠tulo 4: Construindo Paradigmas Cl√°ssicos de Agentes

No cap√≠tulo anterior, exploramos profundamente os modelos de linguagem grandes como o "c√©rebro" dos agentes modernos. Aprendemos sobre sua arquitetura interna Transformer, m√©todos para interagir com eles e suas capacidades e limita√ß√µes. Agora, √© hora de transformar esse conhecimento te√≥rico em pr√°tica e construir agentes com nossas pr√≥prias m√£os.

A capacidade central de um agente moderno reside em sua habilidade de conectar o poder de racioc√≠nio dos modelos de linguagem grandes com o mundo externo. Ele pode compreender autonomamente a inten√ß√£o do usu√°rio, decompor tarefas complexas e alcan√ßar objetivos chamando uma s√©rie de "ferramentas" como interpretadores de c√≥digo, mecanismos de busca e APIs para obter informa√ß√µes e executar opera√ß√µes. No entanto, os agentes n√£o s√£o onipotentes; eles tamb√©m enfrentam desafios do problema de "alucina√ß√£o" inerente aos modelos grandes, loops de racioc√≠nio potenciais em tarefas complexas e uso incorreto de ferramentas, que constituem os limites de capacidade dos agentes.

Para organizar melhor os processos de "pensamento" e "a√ß√£o" dos agentes, a ind√∫stria emergiu com m√∫ltiplos paradigmas arquitet√¥nicos cl√°ssicos. Neste cap√≠tulo, vamos focar nos tr√™s mais representativos e implement√°-los passo a passo do zero:

- **ReAct (Reasoning and Acting):** Um paradigma que combina firmemente "pensamento" e "a√ß√£o", permitindo que os agentes pensem enquanto agem e se ajustem dinamicamente.
- **Plan-and-Solve:** Um paradigma de "pense antes de agir" onde os agentes primeiro geram um plano de a√ß√£o completo e depois o executam estritamente.
- **Reflection (Reflex√£o):** Um paradigma que dota os agentes com capacidades de "reflex√£o", otimizando resultados atrav√©s de autocr√≠tica e corre√ß√£o.

Depois de entender esses conceitos, voc√™ pode perguntar: com muitos frameworks excelentes como LangChain e LlamaIndex j√° dispon√≠veis, por que "reinventar a roda"? A resposta est√° no fato de que, embora os frameworks maduros tenham vantagens significativas em efici√™ncia de engenharia, usar ferramentas altamente abstratas n√£o nos ajuda a entender como os mecanismos de design subjacentes funcionam ou quais benef√≠cios eles oferecem. Em segundo lugar, esse processo exp√µe desafios de engenharia em projetos. Os frameworks lidam com muitas quest√µes para n√≥s, como analisar formatos de sa√≠da do modelo, retentar chamadas de ferramentas que falharam e impedir que os agentes caiam em loops infinitos. Lidar com essas quest√µes em primeira m√£o √© a maneira mais direta de cultivar capacidades de design de sistemas. Finalmente, e mais importante, dominar princ√≠pios de design permite que voc√™ realmente se transforme de um "usu√°rio" de framework para um "criador" de aplica√ß√µes inteligentes. Quando componentes padr√£o n√£o podem atender suas necessidades complexas, voc√™ ter√° a capacidade de personalizar profundamente ou at√© mesmo construir um agente completamente novo do zero.

## 4.1 Prepara√ß√£o do Ambiente e Defini√ß√£o B√°sica de Ferramentas

Antes de come√ßarmos a construir, precisamos configurar o ambiente de desenvolvimento e definir alguns componentes b√°sicos. Isso nos ajudar√° a evitar trabalho repetitivo e focar mais na l√≥gica central ao implementar paradigmas diferentes posteriormente.

### 4.1.1 Instalando Depend√™ncias

A parte pr√°tica deste livro usar√° principalmente a linguagem Python, e recomenda-se Python 3.10 ou superior. Primeiro, certifique-se de ter instalado a biblioteca `openai` para interagir com modelos de linguagem grandes, e a biblioteca `python-dotenv` para gerenciar com seguran√ßa nossas chaves de API.

Execute o seguinte comando no seu terminal:

```bash
pip install openai python-dotenv
```

### 4.1.2 Configurando Chaves de API

Para tornar nosso c√≥digo mais universal, vamos configurar uniformemente informa√ß√µes relacionadas ao servi√ßo do modelo (ID do modelo, chave de API, endere√ßo do servi√ßo) em vari√°veis de ambiente.

1. No diret√≥rio raiz do seu projeto, crie um arquivo chamado `.env`.
2. Neste arquivo, adicione o seguinte conte√∫do. Voc√™ pode apont√°-lo para o servi√ßo oficial da OpenAI ou qualquer servi√ßo local/de terceiros compat√≠vel com a interface OpenAI de acordo com suas necessidades.
3. Se voc√™ realmente n√£o sabe como obt√™-lo, pode consultar a Se√ß√£o [1.2 API Setup](https://datawhalechina.github.io/handy-multi-agent/#/chapter1/1.2.api-setup) em outro tutorial Datawhale.

```bash
# Arquivo .env
LLM_API_KEY="SUA-CHAVE-API"
LLM_MODEL_ID="SEU-MODELO"
LLM_BASE_URL="SUA-URL"
```

Nosso c√≥digo carregar√° automaticamente essas configura√ß√µes deste arquivo.

### 4.1.3 Encapsulando Fun√ß√µes B√°sicas de Chamada LLM

Para tornar a estrutura do c√≥digo mais clara e reutiliz√°vel, vamos definir uma classe cliente LLM dedicada. Esta classe encapsular√° todos os detalhes de intera√ß√£o com servi√ßos de modelo, permitindo que nossa l√≥gica principal foque mais na constru√ß√£o do agente.

```python
import os
from openai import OpenAI
from dotenv import load_dotenv
from typing import List, Dict

# Carregar vari√°veis de ambiente do arquivo .env
load_dotenv()

class HelloAgentsLLM:
    """
    Um cliente LLM personalizado para o livro "Hello Agents".
    √â usado para chamar qualquer servi√ßo compat√≠vel com a interface OpenAI e usa respostas em streaming por padr√£o.
    """
    def __init__(self, model: str = None, apiKey: str = None, baseUrl: str = None, timeout: int = None):
        """
        Inicializa o cliente. Prioriza par√¢metros passados; se n√£o fornecidos, carrega de vari√°veis de ambiente.
        """
        self.model = model or os.getenv("LLM_MODEL_ID")
        apiKey = apiKey or os.getenv("LLM_API_KEY")
        baseUrl = baseUrl or os.getenv("LLM_BASE_URL")
        timeout = timeout or int(os.getenv("LLM_TIMEOUT", 60))

        if not all([self.model, apiKey, baseUrl]):
            raise ValueError("ID do modelo, chave de API e endere√ßo do servi√ßo devem ser fornecidos ou definidos no arquivo .env.")

        self.client = OpenAI(api_key=apiKey, base_url=baseUrl, timeout=timeout)

    def think(self, messages: List[Dict[str, str]], temperature: float = 0) -> str:
        """
        Chama o modelo de linguagem grande para pensar e retorna sua resposta.
        """
        print(f"üß† Chamando o modelo {self.model}...")
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=temperature,
                stream=True,
            )

            # Lidar com resposta em streaming
            print("‚úÖ Resposta do modelo de linguagem grande bem-sucedida:")
            collected_content = []
            for chunk in response:
                content = chunk.choices[0].delta.content or ""
                print(content, end="", flush=True)
                collected_content.append(content)
            print()  # Nova linha ap√≥s o fim da sa√≠da em streaming
            return "".join(collected_content)

        except Exception as e:
            print(f"‚ùå Erro ao chamar a API LLM: {e}")
            return None

# --- Exemplo de Uso do Cliente ---
if __name__ == '__main__':
    try:
        llmClient = HelloAgentsLLM()

        exampleMessages = [
            {"role": "system", "content": "Voc√™ √© um assistente √∫til que escreve c√≥digo Python."},
            {"role": "user", "content": "Escreva um algoritmo quicksort"}
        ]

        print("--- Chamando LLM ---")
        responseText = llmClient.think(exampleMessages)
        if responseText:
            print("\n\n--- Resposta Completa do Modelo ---")
            print(responseText)

    except ValueError as e:
        print(e)


>>>
--- Chamando LLM ---
üß† Chamando o modelo xxxxxx...
‚úÖ Resposta do modelo de linguagem grande bem-sucedida:
Quicksort √© um algoritmo de ordena√ß√£o muito eficiente...
```



## 4.2 ReAct

Ap√≥s preparar o cliente LLM, vamos construir o primeiro e mais cl√°ssico paradigma de agente: **ReAct (Reason + Act)**. O ReAct foi proposto por Shunyu Yao em 2022<sup>[1]</sup>. Sua ideia central √© imitar como os humanos resolvem problemas, combinando explicitamente **Racioc√≠nio** e **A√ß√£o** para formar um loop "pensar-agir-observar".

### 4.2.1 Fluxo de Trabalho do ReAct

Antes do ReAct surgir, os m√©todos principais podiam ser divididos em duas categorias: uma √© o tipo "pensamento puro", como **Chain-of-Thought**, que pode guiar modelos a realizar racioc√≠nio l√≥gico complexo mas n√£o pode interagir com o mundo externo e √© propenso a alucina√ß√µes factuais; a outra √© o tipo "a√ß√£o pura", onde modelos produzem diretamente a√ß√µes para executar mas carecem de capacidades de planejamento e corre√ß√£o de erros.

A engenhosidade do ReAct est√° em reconhecer que **pensamento e a√ß√£o s√£o complementares**. O pensamento guia a a√ß√£o, enquanto os resultados da a√ß√£o por sua vez corrigem o pensamento. Para isso, o paradigma ReAct usa uma engenharia de prompt especial para guiar o modelo de modo que cada passo de sua sa√≠da siga uma trajet√≥ria fixa:

- **Thought (Pensamento):** Este √© o "mon√≥logo interior" do agente. Ele analisa a situa√ß√£o atual, decomp√µe tarefas, formula o pr√≥ximo plano, ou reflete sobre os resultados do passo anterior.
- **Action (A√ß√£o):** Esta √© a a√ß√£o espec√≠fica que o agente decide tomar, geralmente chamando uma ferramenta externa, como `Search['√öltimo telefone da Huawei']`.
- **Observation (Observa√ß√£o):** Este √© o resultado retornado da ferramenta externa ap√≥s executar a `Action`, como um resumo dos resultados de busca ou um valor de retorno de API.

O agente continuar√° repetindo este loop **Thought -> Action -> Observation**, anexando novos resultados de observa√ß√£o ao hist√≥rico para formar um contexto em crescimento cont√≠nuo at√© que ele determine em `Thought` que encontrou a resposta final e ent√£o produz o resultado. Este processo forma uma sinergia poderosa: **o racioc√≠nio torna as a√ß√µes mais propositais, enquanto as a√ß√µes fornecem base factual para o racioc√≠nio.**

Podemos expressar formalmente este processo, como mostrado na Figura 4.1. Especificamente, em cada passo de tempo $t$, a pol√≠tica do agente (ou seja, o modelo de linguagem grande $\pi$) gera o pensamento atual $th_t$ e a√ß√£o $a_t$ com base na quest√£o inicial $q$ e na trajet√≥ria hist√≥rica de todos os passos anteriores "a√ß√£o-observa√ß√£o" $((a_1,o_1),\dots,(a_{t-1},o_{t-1}))$:

$$\left(th_t,a_t\right)=\pi\left(q,(a_1,o_1),\ldots,(a_{t-1},o_{t-1})\right)$$

Subsequentemente, a ferramenta $T$ no ambiente executa a a√ß√£o $a_t$ e retorna um novo resultado de observa√ß√£o $o_t$:

$$o_t = T(a_t)$$

Este loop continua, anexando novos pares $(a_t,o_t)$ ao hist√≥rico at√© que o modelo determine no pensamento $th_t$ que a tarefa est√° completa.

<div align="center">
  <img src="https://raw.githubusercontent.com/datawhalechina/Hello-Agents/main/docs/images/4-figures/4-1.png" alt="Loop sin√©rgico Pensar-Agir-Observar no paradigma ReAct" width="90%"/>
  <p>Figura 4.1 Loop Sin√©rgico Pensar-Agir-Observar no Paradigma ReAct</p>
</div>

Este mecanismo √© particularmente adequado para os seguintes cen√°rios:

- **Tarefas que requerem conhecimento externo**: Como consultar informa√ß√µes em tempo real (clima, not√≠cias, pre√ßos de a√ß√µes), buscar conhecimento em dom√≠nios profissionais, etc.
- **Tarefas que requerem c√°lculos precisos**: Delegar problemas matem√°ticos a ferramentas de calculadora para evitar erros de c√°lculo do LLM.
- **Tarefas que requerem intera√ß√£o com API**: Como operar bancos de dados, chamar a API de um servi√ßo para completar fun√ß√µes espec√≠ficas.

Portanto, vamos construir um agente ReAct com a capacidade de **usar ferramentas externas** para responder perguntas que os modelos de linguagem grandes n√£o podem responder diretamente apenas com sua pr√≥pria base de conhecimento. Por exemplo: "Qual √© o √∫ltimo telefone da Huawei? Quais s√£o seus principais pontos de venda?" Esta quest√£o requer que o agente entenda que precisa buscar online, chamar ferramentas para buscar resultados e resumir a resposta.

### 4.2.2 Defini√ß√£o e Implementa√ß√£o de Ferramentas

Se os modelos de linguagem grandes s√£o o c√©rebro de um agente, ent√£o **Ferramentas** s√£o suas "m√£os e p√©s" para interagir com o mundo externo. Para permitir que o paradigma ReAct realmente resolva os problemas que definimos, o agente precisa da capacidade de chamar ferramentas externas.

Para o objetivo definido nesta se√ß√£o‚Äîresponder perguntas sobre "o √∫ltimo telefone da Huawei"‚Äîprecisamos fornecer ao agente uma ferramenta de busca na web. Aqui escolhemos **SerpApi**, que fornece resultados de busca estruturados do Google atrav√©s de uma API e pode retornar diretamente "caixas de resumo de resposta" ou informa√ß√µes precisas de gr√°ficos de conhecimento.

Primeiro, voc√™ precisa instalar a biblioteca:

```bash
pip install google-search-results
```

Ao mesmo tempo, voc√™ precisa ir ao [site oficial da SerpApi](https://serpapi.com/) para registrar uma conta gratuita, obter sua chave de API e adicion√°-la ao arquivo `.env` no diret√≥rio raiz do nosso projeto:

```bash
# Arquivo .env
# ... (Manter configura√ß√£o LLM anterior)
SERPAPI_API_KEY="SUA_CHAVE_API_SERPAPI"
```

Em seguida, vamos definir e gerenciar esta ferramenta atrav√©s de c√≥digo. Vamos proceder passo a passo: primeiro implementar a funcionalidade central da ferramenta, depois construir um gerenciador de ferramentas geral.

(1) Implementando a L√≥gica Central da Ferramenta de Busca

Uma ferramenta bem definida deve conter os seguintes tr√™s elementos centrais:

1. **Nome**: Um identificador conciso e √∫nico para o agente chamar em `Action`, como `Search`.
2. **Descri√ß√£o**: Uma descri√ß√£o clara em linguagem natural explicando o prop√≥sito desta ferramenta. **Esta √© a parte mais cr√≠tica de todo o mecanismo** porque o modelo de linguagem grande depender√° desta descri√ß√£o para determinar quando usar qual ferramenta.
3. **L√≥gica de Execu√ß√£o**: A fun√ß√£o ou m√©todo que realmente executa a tarefa.

Nossa primeira ferramenta √© a fun√ß√£o `search`, que recebe uma string de consulta e ent√£o retorna os resultados da busca.

```python
from serpapi import SerpApiClient

def search(query: str) -> str:
    """
    Uma ferramenta pr√°tica de busca na web baseada na SerpApi.
    Ela analisa inteligentemente os resultados da busca, priorizando respostas diretas ou informa√ß√µes de gr√°ficos de conhecimento.
    """
    print(f"üîç Executando busca na web [SerpApi]: {query}")
    try:
        api_key = os.getenv("SERPAPI_API_KEY")
        if not api_key:
            return "Erro: SERPAPI_API_KEY n√£o configurada no arquivo .env."

        params = {
            "engine": "google",
            "q": query,
            "api_key": api_key,
            "gl": "cn",  # C√≥digo do pa√≠s
            "hl": "zh-cn", # C√≥digo do idioma
        }

        client = SerpApiClient(params)
        results = client.get_dict()

        # An√°lise inteligente: priorizar encontrar a resposta mais direta
        if "answer_box_list" in results:
            return "\n".join(results["answer_box_list"])
        if "answer_box" in results and "answer" in results["answer_box"]:
            return results["answer_box"]["answer"]
        if "knowledge_graph" in results and "description" in results["knowledge_graph"]:
            return results["knowledge_graph"]["description"]
        if "organic_results" in results and results["organic_results"]:
            # Se n√£o houver resposta direta, retorna resumos dos tr√™s primeiros resultados org√¢nicos
            snippets = [
                f"[{i+1}] {res.get('title', '')}\n{res.get('snippet', '')}"
                for i, res in enumerate(results["organic_results"][:3])
            ]
            return "\n\n".join(snippets)

        return f"Desculpe, nenhuma informa√ß√£o encontrada sobre '{query}'."

    except Exception as e:
        return f"Erro durante a busca: {e}"
```

No c√≥digo acima, ele primeiro verifica se existe `answer_box` (caixa de resumo de resposta do Google) ou informa√ß√µes de `knowledge_graph` (gr√°fico de conhecimento). Se existir, retorna diretamente essas respostas mais precisas. Caso contr√°rio, retorna resumos dos tr√™s primeiros resultados de busca regulares. Esta "an√°lise inteligente" pode fornecer entrada de informa√ß√µes de maior qualidade para o LLM.

(2) Construindo um Executor de Ferramentas Geral

Quando um agente precisa usar m√∫ltiplas ferramentas (por exemplo, al√©m de busca, tamb√©m pode precisar de c√°lculo, consultas a banco de dados, etc.), precisamos de um gerenciador unificado para registrar e despachar essas ferramentas. Para isso, criamos uma classe `ToolExecutor`.

```python
from typing import Dict, Any

class ToolExecutor:
    """
    Um executor de ferramentas respons√°vel por gerenciar e executar ferramentas.
    """
    def __init__(self):
        self.tools: Dict[str, Dict[str, Any]] = {}

    def registerTool(self, name: str, description: str, func: callable):
        """
        Registra uma nova ferramenta na caixa de ferramentas.
        """
        if name in self.tools:
            print(f"Aviso: Ferramenta '{name}' j√° existe e ser√° sobrescrita.")
        self.tools[name] = {"description": description, "func": func}
        print(f"Ferramenta '{name}' registrada.")

    def getTool(self, name: str) -> callable:
        """
        Obt√©m a fun√ß√£o de execu√ß√£o de uma ferramenta pelo nome.
        """
        return self.tools.get(name, {}).get("func")

    def getAvailableTools(self) -> str:
        """
        Obt√©m uma string de descri√ß√£o formatada de todas as ferramentas dispon√≠veis.
        """
        return "\n".join([
            f"- {name}: {info['description']}"
            for name, info in self.tools.items()
        ])

```

(3) Testando

Agora, vamos registrar a ferramenta `search` no `ToolExecutor` e simular uma chamada para verificar se todo o processo funciona corretamente.

```python
# --- Exemplo de Inicializa√ß√£o e Uso de Ferramenta ---
if __name__ == '__main__':
    # 1. Inicializar executor de ferramentas
    toolExecutor = ToolExecutor()

    # 2. Registrar nossa ferramenta de busca pr√°tica
    search_description = "Um mecanismo de busca na web. Use esta ferramenta quando precisar responder perguntas sobre eventos atuais, fatos e informa√ß√µes n√£o encontradas em sua base de conhecimento."
    toolExecutor.registerTool("Search", search_description, search)

    # 3. Imprimir ferramentas dispon√≠veis
    print("\n--- Ferramentas Dispon√≠veis ---")
    print(toolExecutor.getAvailableTools())

    # 4. Chamada de Action do agente, desta vez perguntamos algo em tempo real
    print("\n--- Executar Action: Search['Qual √© o modelo de GPU mais recente da NVIDIA'] ---")
    tool_name = "Search"
    tool_input = "Qual √© o modelo de GPU mais recente da NVIDIA"

    tool_function = toolExecutor.getTool(tool_name)
    if tool_function:
        observation = tool_function(tool_input)
        print("--- Observation ---")
        print(observation)
    else:
        print(f"Erro: Ferramenta chamada '{tool_name}' n√£o encontrada.")

>>>
Ferramenta 'Search' registrada.

--- Ferramentas Dispon√≠veis ---
- Search: Um mecanismo de busca na web. Use esta ferramenta quando precisar responder perguntas sobre eventos atuais, fatos e informa√ß√µes n√£o encontradas em sua base de conhecimento.

--- Executar Action: Search['Qual √© o modelo de GPU mais recente da NVIDIA'] ---
üîç Executando busca na web [SerpApi]: Qual √© o modelo de GPU mais recente da NVIDIA
--- Observation ---
[1] Placas Gr√°ficas S√©rie GeForce RTX 50
As GPUs S√©rie GeForce RTX‚Ñ¢ 50 s√£o alimentadas pela arquitetura NVIDIA Blackwell, trazendo nova jogabilidade para jogadores e criadores. A S√©rie RTX 50 tem poder de computa√ß√£o AI poderoso, trazendo experi√™ncia aprimorada e gr√°ficos mais realistas.

[2] Compare Placas Gr√°ficas S√©rie GeForce √öltima Gera√ß√£o e Gera√ß√£o Anterior
Compare as placas gr√°ficas s√©rie RTX 30 mais recentes com s√©rie RTX 20 anterior, s√©rie GTX 10 e 900. Veja especifica√ß√µes, recursos, suporte t√©cnico, etc.

[3] Placas Gr√°ficas GeForce | NVIDIA
DRIVE AGX. Poder de computa√ß√£o poderoso dentro do ve√≠culo para sistemas de ve√≠culos inteligentes impulsionados por AI ¬∑ Clara AGX. Computa√ß√£o AI para dispositivos m√©dicos inovadores e imagem. Jogos e Cria√ß√£o. GeForce. Explore placas gr√°ficas, solu√ß√µes de jogos, AI...
```

At√© agora, equipamos o agente com uma ferramenta `Search` que se conecta √† internet do mundo real, fornecendo uma base s√≥lida para o loop ReAct subsequente.



### 4.2.3 Implementa√ß√£o em C√≥digo do Agente ReAct

Agora, vamos montar todos os componentes independentes‚Äîo cliente LLM e o executor de ferramentas‚Äîpara construir um agente ReAct completo. Vamos encapsular sua l√≥gica central atrav√©s de uma classe `ReActAgent`. Para facilitar o entendimento, vamos dividir o processo de implementa√ß√£o desta classe nas seguintes partes-chave para explica√ß√£o.

(1) Design do Prompt do Sistema

O prompt √© a pedra angular de todo o mecanismo ReAct, fornecendo instru√ß√µes operacionais para o modelo de linguagem grande. Precisamos projetar cuidadosamente um modelo que inserir√° dinamicamente ferramentas dispon√≠veis, perguntas dos usu√°rios e o hist√≥rico de intera√ß√£o de passos intermedi√°rios.

```bash
# Modelo de Prompt ReAct
REACT_PROMPT_TEMPLATE = """
Observe que voc√™ √© um assistente inteligente capaz de chamar ferramentas externas.

As ferramentas dispon√≠veis s√£o as seguintes:
{tools}

Por favor, responda estritamente no seguinte formato:

Thought: Seu processo de pensamento, usado para analisar problemas, decompor tarefas e planejar a pr√≥xima a√ß√£o.
Action: A a√ß√£o que voc√™ decide tomar, deve estar em um dos seguintes formatos:
- {{tool_name}}[{{tool_input}}]`: Chamar uma ferramenta dispon√≠vel.
- `Finish[resposta final]`: Quando voc√™ acredita que obteve a resposta final.
- Quando voc√™ tiver coletado informa√ß√µes suficientes para responder √† pergunta final do usu√°rio, voc√™ deve usar `finish(answer="...")` ap√≥s o campo Action: para produzir a resposta final.

Agora, por favor comece a resolver o seguinte problema:
Question: {question}
History: {history}
"""
```

Este modelo define a especifica√ß√£o para intera√ß√£o entre o agente e o LLM:

- **Defini√ß√£o de Papel**: "Voc√™ √© um assistente inteligente capaz de chamar ferramentas externas" define o papel do LLM.
- **Lista de Ferramentas (`{tools}`)**: Informa ao LLM quais "m√£os e p√©s" ele tem dispon√≠veis.
- **Conven√ß√£o de Formato (`Thought`/`Action`)**: Esta √© a parte mais importante, for√ßando a sa√≠da do LLM a ser estruturada para que possamos analisar precisamente sua inten√ß√£o atrav√©s de c√≥digo.
- **Contexto Din√¢mico (`{question}`/`{history}`)**: Injeta a pergunta original do usu√°rio e o hist√≥rico de intera√ß√£o continuamente acumulado, permitindo que o LLM tome decis√µes baseadas no contexto completo.

(2) Implementa√ß√£o do Loop Central

O n√∫cleo do `ReActAgent` √© um loop que continuamente "formata prompt -> chama LLM -> executa a√ß√£o -> integra resultados" at√© que a tarefa seja completa ou o limite m√°ximo de passos seja atingido.

```python
class ReActAgent:
    def __init__(self, llm_client: HelloAgentsLLM, tool_executor: ToolExecutor, max_steps: int = 5):
        self.llm_client = llm_client
        self.tool_executor = tool_executor
        self.max_steps = max_steps
        self.history = []

    def run(self, question: str):
        """
        Executa o agente ReAct para responder uma pergunta.
        """
        self.history = [] # Resetar hist√≥rico para cada execu√ß√£o
        current_step = 0

        while current_step < self.max_steps:
            current_step += 1
            print(f"--- Passo {current_step} ---")

            # 1. Formatar prompt
            tools_desc = self.tool_executor.getAvailableTools()
            history_str = "\n".join(self.history)
            prompt = REACT_PROMPT_TEMPLATE.format(
                tools=tools_desc,
                question=question,
                history=history_str
            )

            # 2. Chamar LLM para pensar
            messages = [{"role": "user", "content": prompt}]
            response_text = self.llm_client.think(messages=messages)

            if not response_text:
                print("Erro: LLM falhou ao retornar uma resposta v√°lida.")
                break

            # ... (Passos subsequentes de an√°lise, execu√ß√£o e integra√ß√£o)

```

O m√©todo `run` √© o ponto de entrada do agente. Seu loop `while` constitui o corpo principal do paradigma ReAct, e o par√¢metro `max_steps` √© uma v√°lvula de seguran√ßa importante para evitar que o agente caia em um loop infinito e esgote recursos.

(3) Implementa√ß√£o do Analisador de Sa√≠da

O LLM retorna texto simples, e precisamos extrair precisamente `Thought` e `Action` dele. Isso √© realizado atrav√©s de v√°rias fun√ß√µes auxiliares de an√°lise, que normalmente usam express√µes regulares.

```python
# (Estes m√©todos fazem parte da classe ReActAgent)
    def _parse_output(self, text: str):
        """Analisa a sa√≠da do LLM para extrair Thought e Action."""
        thought_match = re.search(r"Thought: (.*)", text)
        action_match = re.search(r"Action: (.*)", text)
        thought = thought_match.group(1).strip() if thought_match else None
        action = action_match.group(1).strip() if action_match else None
        return thought, action

    def _parse_action(self, action_text: str):
        """Analisa a string Action para extrair nome da ferramenta e entrada."""
        match = re.match(r"(\w+)\[(.*)\]", action_text)
        if match:
            return match.group(1), match.group(2)
        return None, None
```

- `_parse_output`: Respons√°vel por separar as duas partes principais `Thought` e `Action` da resposta completa do LLM.
- `_parse_action`: Respons√°vel por analisar ainda mais a string `Action`, por exemplo, extraindo o nome da ferramenta `Search` e a entrada da ferramenta `√öltimo telefone da Huawei` de `Search[√öltimo telefone da Huawei]`.

(4) Invoca√ß√£o e Execu√ß√£o de Ferramenta

```python
# (Esta l√≥gica est√° dentro do loop while do m√©todo run)
            # 3. Analisar sa√≠da do LLM
            thought, action = self._parse_output(response_text)

            if thought:
                print(f"Thought: {thought}")

            if not action:
                print("Aviso: Falha ao analisar Action v√°lida, processo terminado.")
                break

            # 4. Executar Action
            if action.startswith("Finish"):
                # Se for uma instru√ß√£o Finish, extrair resposta final e terminar
                final_answer = re.match(r"Finish\[(.*)\]", action).group(1)
                print(f"üéâ Resposta Final: {final_answer}")
                return final_answer

            tool_name, tool_input = self._parse_action(action)
            if not tool_name or not tool_input:
                # ... Lidar com formato de Action inv√°lido ...
                continue

            print(f"üé¨ Action: {tool_name}[{tool_input}]")

            tool_function = self.tool_executor.getTool(tool_name)
            if not tool_function:
                observation = f"Erro: Ferramenta chamada '{tool_name}' n√£o encontrada."
            else:
                observation = tool_function(tool_input) # Chamar ferramenta real

```

Este c√≥digo √© o centro de execu√ß√£o de `Action`. Ele primeiro verifica se √© uma instru√ß√£o `Finish`; se for, o processo termina. Caso contr√°rio, obt√©m a fun√ß√£o de ferramenta correspondente atrav√©s de `tool_executor` e a executa para obter a `observation`.

(5) Integra√ß√£o de Resultados de Observa√ß√£o

O √∫ltimo passo, e a chave para formar um loop fechado, √© adicionar a pr√≥pria `Action` e a `Observation` ap√≥s a execu√ß√£o da ferramenta de volta ao hist√≥rico, fornecendo novo contexto para o pr√≥ximo loop.

```python
# (Esta l√≥gica segue a invoca√ß√£o da ferramenta, no final do loop while)
            print(f"üëÄ Observation: {observation}")

            # Adicionar Action e Observation desta rodada ao hist√≥rico
            self.history.append(f"Action: {action}")
            self.history.append(f"Observation: {observation}")

        # Loop termina
        print("N√∫mero m√°ximo de passos atingido, processo terminado.")
        return None
```

Ao anexar `Observation` a `self.history`, o agente pode "ver" os resultados da a√ß√£o anterior ao gerar o prompt na pr√≥xima rodada, e conduzir novo pensamento e planejamento de acordo.

(6) Inst√¢ncia de Execu√ß√£o e An√°lise

Combinando todas as partes acima, obtemos a classe `ReActAgent` completa. A inst√¢ncia de c√≥digo de execu√ß√£o completa pode ser encontrada na pasta `code` do reposit√≥rio de c√≥digo que acompanha este livro.

Abaixo est√° um registro de execu√ß√£o real:

```
Ferramenta 'Search' registrada.

--- Passo 1 ---
üß† Chamando o modelo xxxxxx...
‚úÖ Resposta do modelo de linguagem grande bem-sucedida:
Thought: Para responder esta pergunta, preciso buscar o modelo de telefone mais recente lan√ßado pela Huawei e seus recursos principais. Esta informa√ß√£o pode estar fora da minha base de conhecimento existente, ent√£o preciso usar um mecanismo de busca para obter os dados mais recentes.
Action: Search[modelo de telefone mais recente da Huawei e principais pontos de venda]
ü§î Thought: Para responder esta pergunta, preciso buscar o modelo de telefone mais recente lan√ßado pela Huawei e seus recursos principais. Esta informa√ß√£o pode estar fora da minha base de conhecimento existente, ent√£o preciso usar um mecanismo de busca para obter os dados mais recentes.
üé¨ Action: Search[modelo de telefone mais recente da Huawei e principais pontos de venda]
üîç Executando busca na web [SerpApi]: modelo de telefone mais recente da Huawei e principais pontos de venda
üëÄ Observation: [1] Telefones Huawei - Site Oficial da Huawei
Smartphones ; S√©rie Mate. Flagship Extraordin√°rio ¬∑ HUAWEI Mate XTs. Mestre Extraordin√°rio ; S√©rie Pura. Imagem Pioneira ¬∑ HUAWEI Pura 80 Pro+ ; S√©rie Pocket. Novo Cap√≠tulo Est√©tico. HUAWEI Pocket 2 ; S√©rie nova. Retrato Profissional.

[2] Qual telefone Huawei tem melhor custo-benef√≠cio em 2025? Recomenda√ß√µes de telefones Huawei e an√°lise de mercado...
Agora o maior ponto de venda dos telefones Huawei √© apenas o sistema HarmonyOS, e a f√© controversa na marca. Aqui est√£o v√°rios telefones da s√©rie Huawei que valem a pena comprar, escolha de acordo com diferentes or√ßamentos:. O mais popular da Huawei, tamb√©m equipado com...

[3] Qual novo telefone Huawei tem melhor custo-benef√≠cio em 2025? 10 recomenda√ß√µes de novos telefones Huawei
Escolher Huawei principalmente recomenda telefones de ponta, Mate 70 e s√©rie Pura 70 s√£o os modelos flagship mais recentes. HUAWEI Mate 70. As vantagens s√£o, configura√ß√£o de fotografia ainda √© de n√≠vel superior, cobertura de alcance focal completo, adequado para fotografia profissional, excelente acabamento, resist√™ncia a quedas ao ar livre...

--- Passo 2 ---
üß† Chamando o modelo xxxxxx...
‚úÖ Resposta do modelo de linguagem grande bem-sucedida:
Thought: Com base nos resultados da busca, os modelos flagship mais recentes da Huawei incluem Mate 70 e Pura 80 Pro+. Para determinar o modelo "mais recente" e seus principais pontos de venda, vou focar nesta informa√ß√£o. Dos links fornecidos, tanto a s√©rie Mate 70 quanto a Pura 80 Pro+ s√£o produtos lan√ßados recentemente, mas qual √© "mais recente" precisa de confirma√ß√£o adicional. Enquanto isso, posso extrair seus principais pontos de venda desta informa√ß√£o.
Action: Finish[De acordo com as informa√ß√µes mais recentes, os telefones mais recentes da Huawei podem ser HUAWEI Pura 80 Pro+ ou HUAWEI Mate 70. Entre eles, os principais pontos de venda do HUAWEI Mate 70 incluem configura√ß√£o de fotografia de n√≠vel superior, cobertura de alcance focal completo, adequado para fotografia profissional, excelente acabamento e boa resist√™ncia a quedas ao ar livre. Enquanto o HUAWEI Pura 80 Pro+ enfatiza tecnologia de imagem pioneira.]
ü§î Thought: Com base nos resultados da busca, os modelos flagship mais recentes da Huawei incluem Mate 70 e Pura 80 Pro+. Para determinar o modelo "mais recente" e seus principais pontos de venda, vou focar nesta informa√ß√£o. Dos links fornecidos, tanto a s√©rie Mate 70 quanto a Pura 80 Pro+ s√£o produtos lan√ßados recentemente, mas qual √© "mais recente" precisa de confirma√ß√£o adicional. Enquanto isso, posso extrair seus principais pontos de venda desta informa√ß√£o.
üéâ Resposta Final: De acordo com as informa√ß√µes mais recentes, os telefones mais recentes da Huawei podem ser HUAWEI Pura 80 Pro+ ou HUAWEI Mate 70. Entre eles, os principais pontos de venda do HUAWEI Mate 70 incluem configura√ß√£o de fotografia de n√≠vel superior, cobertura de alcance focal completo, adequado para fotografia profissional, excelente acabamento e boa resist√™ncia a quedas ao ar livre. Enquanto o HUAWEI Pura 80 Pro+ enfatiza tecnologia de imagem pioneira.
```

Da sa√≠da acima, podemos ver que o agente demonstra claramente sua cadeia de pensamento: primeiro percebe que seu conhecimento √© insuficiente e precisa usar a ferramenta de busca; ent√£o, raciocina e resume com base nos resultados da busca, chegando √† resposta final em dois passos.

Vale notar que como o conhecimento do modelo e as informa√ß√µes da internet est√£o constantemente sendo atualizados, seus resultados de execu√ß√£o podem n√£o ser exatamente os mesmos que este. Em 8 de setembro de 2025, quando esta se√ß√£o foi escrita, o HUAWEI Mate 70 e o HUAWEI Pura 80 Pro+ mencionados nos resultados de busca eram de fato os telefones da s√©rie flagship mais recentes da Huawei naquela √©poca. Isso demonstra plenamente a capacidade poderosa do paradigma ReAct em lidar com quest√µes sens√≠veis ao tempo.

### 4.2.4 Caracter√≠sticas, Limita√ß√µes e T√©cnicas de Depura√ß√£o do ReAct

Ao implementar um agente ReAct em primeira m√£o, n√£o apenas dominamos seu fluxo de trabalho mas tamb√©m devemos ter uma compreens√£o mais profunda de seus mecanismos internos. Qualquer paradigma t√©cnico tem seus destaques e √°reas para melhoria; esta se√ß√£o resumir√° o ReAct.

(1) Principais Caracter√≠sticas do ReAct

1. **Alta Interpretabilidade**: Uma das maiores vantagens do ReAct √© a transpar√™ncia. Atrav√©s da cadeia de `Thought`, podemos ver claramente a "jornada mental" do agente em cada passo‚Äîpor que escolheu esta ferramenta e o que planeja fazer a seguir. Isso √© crucial para entender, confiar e depurar o comportamento do agente.
2. **Capacidade de Planejamento Din√¢mico e Corre√ß√£o de Erros**: Diferente de paradigmas que geram planos completos de uma vez, o ReAct √© "dar um passo, olhar um passo." Ele ajusta dinamicamente os subsequentes `Thought` e `Action` com base na `Observation` obtida do mundo externo em cada passo. Se os resultados de busca anteriores forem insatisfat√≥rios, ele pode corrigir os termos de busca no pr√≥ximo passo e tentar novamente.
3. **Capacidade de Sinergia de Ferramentas**: O paradigma ReAct combina naturalmente a capacidade de racioc√≠nio dos modelos de linguagem grandes com a capacidade de execu√ß√£o de ferramentas externas. Os LLMs s√£o respons√°veis por estrat√©gias (planejamento e racioc√≠nio), as ferramentas s√£o respons√°veis por resolver problemas espec√≠ficos (buscar, calcular), e os dois trabalham sinergicamente, quebrando as limita√ß√µes inerentes dos LLMs √∫nicos em atualidade do conhecimento, precis√£o computacional, etc.

(2) Limita√ß√µes Inerentes do ReAct

1. **Forte Depend√™ncia das Pr√≥prias Capacidades do LLM**: O sucesso do processo ReAct depende muito das capacidades abrangentes do LLM subjacente. Se a capacidade de racioc√≠nio l√≥gico, capacidade de seguir instru√ß√µes ou capacidade de sa√≠da formatada do LLM for insuficiente, √© f√°cil produzir planejamento errado na fase de `Thought` ou gerar instru√ß√µes que n√£o se conformam ao formato na fase de `Action`, causando a interrup√ß√£o de todo o processo.
2. **Problemas de Efici√™ncia de Execu√ß√£o**: Devido √† sua natureza passo a passo, completar uma tarefa geralmente requer m√∫ltiplas chamadas ao LLM. Cada chamada √© acompanhada por lat√™ncia de rede e custo computacional. Para tarefas complexas que requerem muitos passos, este loop serial "pensar-agir" pode levar a alto tempo total e custo.
3. **Fragilidade do Prompt**: A opera√ß√£o est√°vel de todo o mecanismo √© constru√≠da sobre um modelo de prompt cuidadosamente projetado. Qualquer mudan√ßa menor no modelo, mesmo diferen√ßas na reda√ß√£o, pode afetar o comportamento do LLM. Adicionalmente, nem todos os modelos podem seguir consistentemente formatos predefinidos, aumentando a incerteza em aplica√ß√µes pr√°ticas.
4. **Pode Cair em √ìtimos Locais**: O modo de tomada de decis√£o passo a passo significa que o agente carece de um plano global de longo prazo. Ele pode escolher um caminho que parece correto a curto prazo mas n√£o √© √≥timo a longo prazo devido √† `Observation` imediata, ou at√© mesmo cair em um loop "girando no mesmo lugar" em alguns casos.

(3) T√©cnicas de Depura√ß√£o

Quando seu agente ReAct constru√≠do se comporta inesperadamente, voc√™ pode depurar dos seguintes aspectos:

- **Verificar Prompt Completo**: Antes de cada chamada ao LLM, imprima o prompt formatado final completo contendo todo o hist√≥rico. Esta √© a maneira mais direta de rastrear a fonte das decis√µes do LLM.
- **Analisar Sa√≠da Bruta**: Quando a an√°lise de sa√≠da falha (por exemplo, express√µes regulares n√£o corresponderam a `Action`), certifique-se de imprimir o texto bruto n√£o processado retornado pelo LLM. Isso pode ajud√°-lo a determinar se o LLM n√£o seguiu o formato ou se sua l√≥gica de an√°lise est√° errada.
- **Verificar Entrada e Sa√≠da da Ferramenta**: Verifique se o `tool_input` gerado pelo agente est√° no formato esperado pela fun√ß√£o da ferramenta, e tamb√©m certifique-se de que a `observation` retornada pela ferramenta est√° em um formato que o agente pode entender e processar.
- **Ajustar Exemplos no Prompt (Few-shot Prompting)**: Se o modelo frequentemente comete erros, voc√™ pode adicionar um ou dois casos completos de sucesso "Thought-Action-Observation" no prompt para guiar o modelo a seguir melhor suas instru√ß√µes atrav√©s de exemplos.
- **Tentar Modelos ou Par√¢metros Diferentes**: Mudar para um modelo mais capaz ou ajustar o par√¢metro `temperature` (geralmente definido para 0 para garantir determinismo de sa√≠da) √†s vezes pode resolver o problema diretamente.

## 4.3 Plan-and-Solve

Ap√≥s dominar o ReAct, este paradigma de agente reativo e de tomada de decis√£o passo a passo, vamos explorar a seguir um m√©todo com estilo muito diferente mas igualmente poderoso: **Plan-and-Solve**. Como o nome sugere, este paradigma divide explicitamente o processamento de tarefas em duas fases: **Planejar primeiro, depois Resolver**.

Se o ReAct √© como um detetive experiente que raciocina passo a passo com base em pistas na cena (Observation) e ajusta a dire√ß√£o da investiga√ß√£o a qualquer momento; ent√£o Plan-and-Solve √© mais como um arquiteto que deve primeiro desenhar um projeto completo (Plan) antes de come√ßar a constru√ß√£o, depois construir estritamente de acordo com o projeto (Solve). De fato, muitos modos Agent de ferramentas de modelos grandes que usamos agora incorporam este padr√£o de design.

### 4.3.1 Princ√≠pio de Funcionamento do Plan-and-Solve

Plan-and-Solve Prompting foi proposto por Lei Wang em 2023<sup>[2]</sup>. Sua motiva√ß√£o central √© resolver o problema de que chain-of-thought facilmente "sai dos trilhos" ao lidar com problemas complexos de m√∫ltiplos passos.

Diferente do ReAct, que integra pensamento e a√ß√£o em cada passo, Plan-and-Solve desacopla todo o processo em duas fases centrais, como mostrado na Figura 4.2:

1. **Fase de Planejamento**: Primeiro, o agente recebe a pergunta completa do usu√°rio. Sua primeira tarefa n√£o √© resolver diretamente o problema ou chamar ferramentas, mas **decompor o problema e formular um plano de a√ß√£o claro, passo a passo**. Este plano em si √© o produto de uma chamada ao modelo de linguagem grande.
2. **Fase de Resolu√ß√£o**: Ap√≥s obter o plano completo, o agente entra na fase de execu√ß√£o. Ele **executar√° estritamente de acordo com os passos do plano, um por um**. A execu√ß√£o de cada passo pode ser uma chamada LLM independente ou processamento dos resultados do passo anterior, at√© que todos os passos do plano sejam completados e a resposta final seja obtida.

Esta estrat√©gia "planejar antes de agir" permite que o agente mantenha maior consist√™ncia de objetivos ao lidar com tarefas complexas que requerem planejamento de longo prazo, evitando se perder em passos intermedi√°rios.

Podemos expressar formalmente este processo de duas fases. Primeiro, o modelo de planejamento $\pi_{\text{plan}}$ gera um plano $P = (p_1, p_2, \dots, p_n)$ contendo $n$ passos com base na pergunta original $q$:

$$
P = \pi_{\text{plan}}(q)
$$

Subsequentemente, na fase de execu√ß√£o, o modelo de execu√ß√£o $\pi_{\text{solve}}$ completar√° os passos do plano um por um. Para o $i$-√©simo passo, a gera√ß√£o de sua solu√ß√£o $s_i$ depender√° da pergunta original $q$, do plano completo $P$, e dos resultados de execu√ß√£o de todos os passos anteriores $(s_1, \dots, s_{i-1})$:

$$
s_i = \pi_{\text{solve}}(q, P, (s_1, \dots, s_{i-1}))
$$

A resposta final √© o resultado de execu√ß√£o do √∫ltimo passo $s_n$.

<div align="center">
  <img src="https://raw.githubusercontent.com/datawhalechina/Hello-Agents/main/docs/images/4-figures/4-2.png" alt="Fluxo de trabalho de duas fases do paradigma Plan-and-Solve" width="90%"/>
  <p>Figura 4.2 Fluxo de Trabalho de Duas Fases do Paradigma Plan-and-Solve</p>
</div>

Plan-and-Solve √© especialmente adequado para tarefas complexas com forte estrutura que podem ser claramente decompostas, como:

- **Problemas de palavras matem√°ticos de m√∫ltiplos passos**: Precisa primeiro listar passos de c√°lculo, depois resolver um por um.
- **Escrita de relat√≥rios integrando m√∫ltiplas fontes de informa√ß√£o**: Precisa primeiro planejar a estrutura do relat√≥rio (introdu√ß√£o, fonte de dados A, fonte de dados B, resumo), depois preencher o conte√∫do um por um.
- **Tarefas de gera√ß√£o de c√≥digo**: Precisa primeiro conceber a estrutura de fun√ß√µes, classes e m√≥dulos, depois implementar um por um.

### 4.3.2 Fase de Planejamento

Para destacar as vantagens do paradigma Plan-and-Solve em tarefas de racioc√≠nio estruturado, n√£o usaremos ferramentas mas completaremos uma tarefa de racioc√≠nio atrav√©s de design de prompt.

A caracter√≠stica deste tipo de tarefa √© que a resposta n√£o pode ser obtida atrav√©s de uma √∫nica consulta ou c√°lculo; o problema deve primeiro ser decomposto em uma s√©rie de sub-passos logicamente coerentes, depois resolvidos em ordem. Isso aproveita precisamente a capacidade central do Plan-and-Solve de "planejar primeiro, executar depois."

**Nosso problema alvo √©:** "Uma loja de frutas vendeu 15 ma√ß√£s na segunda-feira. O n√∫mero de ma√ß√£s vendidas na ter√ßa-feira foi o dobro da segunda-feira. O n√∫mero vendido na quarta-feira foi 5 a menos que ter√ßa-feira. Quantas ma√ß√£s foram vendidas no total nesses tr√™s dias?"

Este problema n√£o √© particularmente dif√≠cil para modelos de linguagem grandes, mas cont√©m uma cadeia l√≥gica clara para refer√™ncia. Para alguns quebra-cabe√ßas l√≥gicos reais, se o modelo grande n√£o pode raciocinar respostas precisas com alta qualidade, voc√™ pode consultar este padr√£o de design para projetar seu pr√≥prio Agente para completar a tarefa. O agente precisa:

1. **Fase de Planejamento**: Primeiro, decompor o problema em tr√™s passos de c√°lculo independentes (calcular vendas de ter√ßa-feira, calcular vendas de quarta-feira, calcular vendas totais).
2. **Fase de Execu√ß√£o**: Ent√£o, seguir estritamente o plano, executar c√°lculos passo a passo, e usar o resultado de cada passo como entrada para o pr√≥ximo passo, finalmente obtendo o total.

O objetivo da fase de planejamento √© ter o modelo de linguagem grande receber o problema original e produzir um plano claro, passo a passo. Este plano deve ser estruturado para que nosso c√≥digo possa facilmente analisar e executar um por um. Portanto, o prompt que projetamos precisa dizer claramente ao modelo seu papel e tarefa e fornecer um exemplo do formato de sa√≠da.

````python
PLANNER_PROMPT_TEMPLATE = """
Voc√™ √© um especialista em planejamento de IA de primeira linha. Sua tarefa √© decompor problemas complexos colocados pelos usu√°rios em um plano de a√ß√£o consistindo de m√∫ltiplos passos simples.
Por favor, certifique-se de que cada passo no plano seja uma subtarefa independente e execut√°vel e seja estritamente organizado em ordem l√≥gica.
Sua sa√≠da deve ser uma lista Python, onde cada elemento √© uma string descrevendo uma subtarefa.

Question: {question}

Por favor, produza estritamente seu plano no seguinte formato, com ```python e ``` como prefixo e sufixo sendo necess√°rios:
```python
["Passo 1", "Passo 2", "Passo 3", ...]
```
"""
````

Este prompt garante qualidade e estabilidade de sa√≠da atrav√©s dos seguintes pontos:
- **Configura√ß√£o de Papel**: "Especialista em planejamento de IA de primeira linha" ativa as capacidades profissionais do modelo.
- **Descri√ß√£o da Tarefa**: Define claramente o objetivo de "decompor problemas."
- **Restri√ß√£o de Formato**: For√ßa a sa√≠da a ser uma string em formato de lista Python, o que simplifica muito o trabalho de an√°lise de c√≥digo subsequente, tornando-o mais est√°vel e confi√°vel do que analisar linguagem natural.

Em seguida, encapsulamos esta l√≥gica de prompt em uma classe `Planner`, que tamb√©m √© nosso planejador.

```python
# Assume que a classe HelloAgentsLLM em llm_client.py j√° est√° definida
# from llm_client import HelloAgentsLLM

class Planner:
    def __init__(self, llm_client):
        self.llm_client = llm_client

    def plan(self, question: str) -> list[str]:
        """
        Gera um plano de a√ß√£o baseado na pergunta do usu√°rio.
        """
        prompt = PLANNER_PROMPT_TEMPLATE.format(question=question)

        # Para gerar um plano, constru√≠mos uma lista de mensagens simples
        messages = [{"role": "user", "content": prompt}]

        print("--- Gerando Plano ---")
        # Usa sa√≠da em streaming para obter o plano completo
        response_text = self.llm_client.think(messages=messages) or ""

        print(f"‚úÖ Plano Gerado:\n{response_text}")

        # Analisa a sa√≠da de string da lista pelo LLM
        try:
            # Encontra conte√∫do entre ```python e ```
            plan_str = response_text.split("```python")[1].split("```")[0].strip()
            # Usa ast.literal_eval para executar com seguran√ßa a string e convert√™-la em uma lista Python
            plan = ast.literal_eval(plan_str)
            return plan if isinstance(plan, list) else []
        except (ValueError, SyntaxError, IndexError) as e:
            print(f"‚ùå Erro ao analisar plano: {e}")
            print(f"Resposta bruta: {response_text}")
            return []
        except Exception as e:
            print(f"‚ùå Erro desconhecido ocorreu ao analisar plano: {e}")
            return []
```

### 4.3.3 Executor e Gerenciamento de Estado

Ap√≥s o planejador (`Planner`) gerar um projeto de a√ß√£o claro, precisamos de um executor (`Executor`) para completar as tarefas no plano uma por uma. O executor n√£o √© apenas respons√°vel por chamar o modelo de linguagem grande para resolver cada sub-problema mas tamb√©m desempenha um papel crucial: **gerenciamento de estado**. Ele deve registrar os resultados de execu√ß√£o de cada passo e fornec√™-los como contexto para passos subsequentes, garantindo que a informa√ß√£o flua suavemente por toda a cadeia de tarefas.

O prompt do executor √© diferente do planejador. Seu objetivo n√£o √© decompor problemas mas **focar em resolver o passo atual baseado no contexto existente**. Portanto, o prompt precisa incluir as seguintes informa√ß√µes-chave:

- **Pergunta Original**: Garante que o modelo sempre entenda o objetivo final.
- **Plano Completo**: Permite que o modelo entenda a posi√ß√£o do passo atual em toda a tarefa.
- **Passos Hist√≥ricos e Resultados**: Fornece trabalho completado at√© agora como entrada direta para o passo atual.
- **Passo Atual**: Instrui claramente o modelo sobre qual tarefa espec√≠fica ele precisa resolver agora.

```python
EXECUTOR_PROMPT_TEMPLATE = """
Voc√™ √© um especialista em execu√ß√£o de IA de primeira linha. Sua tarefa √© seguir estritamente o plano dado e resolver o problema passo a passo.
Voc√™ receber√° a pergunta original, o plano completo, e os passos e resultados completados at√© agora.
Por favor, foque em resolver o "passo atual" e produza apenas a resposta final para aquele passo, sem quaisquer explica√ß√µes ou di√°logos adicionais.

# Pergunta Original:
{question}

# Plano Completo:
{plan}

# Passos Hist√≥ricos e Resultados:
{history}

# Passo Atual:
{current_step}

Por favor, produza apenas a resposta para o "passo atual":
"""
```

Encapsulamos a l√≥gica de execu√ß√£o na classe `Executor`. Esta classe percorrer√° o plano, chamar√° o LLM e manter√° um hist√≥rico (estado).

```python
class Executor:
    def __init__(self, llm_client):
        self.llm_client = llm_client

    def execute(self, question: str, plan: list[str]) -> str:
        """
        Executa passo a passo de acordo com o plano e resolve o problema.
        """
        history = "" # String para armazenar passos hist√≥ricos e resultados

        print("\n--- Executando Plano ---")

        for i, step in enumerate(plan):
            print(f"\n-> Executando passo {i+1}/{len(plan)}: {step}")

            prompt = EXECUTOR_PROMPT_TEMPLATE.format(
                question=question,
                plan=plan,
                history=history if history else "Nenhum", # Se for o primeiro passo, hist√≥rico est√° vazio
                current_step=step
            )

            messages = [{"role": "user", "content": prompt}]

            response_text = self.llm_client.think(messages=messages) or ""

            # Atualiza hist√≥rico para o pr√≥ximo passo
            history += f"Passo {i+1}: {step}\nResultado: {response_text}\n\n"

            print(f"‚úÖ Passo {i+1} completado, resultado: {response_text}")

        # Ap√≥s o loop terminar, a resposta do √∫ltimo passo √© a resposta final
        final_answer = response_text
        return final_answer
```

Agora constru√≠mos separadamente o `Planner` respons√°vel por "planejar" e o `Executor` respons√°vel por "executar." O √∫ltimo passo √© integrar esses dois componentes em um agente unificado `PlanAndSolveAgent` e dar a ele capacidades completas de resolu√ß√£o de problemas. Vamos criar uma classe principal `PlanAndSolveAgent` cuja responsabilidade √© muito clara: receber um cliente LLM, inicializar planejador e executor internos, e fornecer um m√©todo `run` simples para iniciar todo o processo.

```python
class PlanAndSolveAgent:
    def __init__(self, llm_client):
        """
        Inicializa o agente e cria inst√¢ncias de planejador e executor.
        """
        self.llm_client = llm_client
        self.planner = Planner(self.llm_client)
        self.executor = Executor(self.llm_client)

    def run(self, question: str):
        """
        Executa o processo completo do agente: planejar primeiro, depois executar.
        """
        print(f"\n--- Come√ßando a Processar Pergunta ---\nPergunta: {question}")

        # 1. Chamar planejador para gerar plano
        plan = self.planner.plan(question)

        # Verifica se o plano foi gerado com sucesso
        if not plan:
            print("\n--- Tarefa Terminada --- \nIncapaz de gerar plano de a√ß√£o v√°lido.")
            return

        # 2. Chamar executor para executar plano
        final_answer = self.executor.execute(question, plan)

        print(f"\n--- Tarefa Completada ---\nResposta Final: {final_answer}")
```

O design desta classe `PlanAndSolveAgent` incorpora o princ√≠pio de "composi√ß√£o sobre heran√ßa." Ela n√£o cont√©m l√≥gica complexa em si mas age como um orquestrador, chamando claramente seus componentes internos para completar tarefas.

### 4.3.4 Inst√¢ncia de Execu√ß√£o e An√°lise

O c√≥digo completo tamb√©m pode ser encontrado na pasta `code` do reposit√≥rio de c√≥digo que acompanha este livro; aqui demonstramos apenas os resultados finais.

````bash
--- Come√ßando a Processar Pergunta ---
Pergunta: Uma loja de frutas vendeu 15 ma√ß√£s na segunda-feira. O n√∫mero de ma√ß√£s vendidas na ter√ßa-feira foi o dobro da segunda-feira. O n√∫mero vendido na quarta-feira foi 5 a menos que ter√ßa-feira. Quantas ma√ß√£s foram vendidas no total nesses tr√™s dias?
--- Gerando Plano ---
üß† Chamando o modelo xxxx...
‚úÖ Resposta do modelo de linguagem grande bem-sucedida:
```python
["Calcular vendas de ma√ß√£s na segunda-feira: 15", "Calcular vendas de ma√ß√£s na ter√ßa-feira: quantidade da segunda-feira √ó 2 = 15 √ó 2 = 30", "Calcular vendas de ma√ß√£s na quarta-feira: quantidade da ter√ßa-feira - 5 = 30 - 5 = 25", "Calcular vendas totais de tr√™s dias: segunda-feira + ter√ßa-feira + quarta-feira = 15 + 30 + 25 = 70"]
```
‚úÖ Plano Gerado:
```python
["Calcular vendas de ma√ß√£s na segunda-feira: 15", "Calcular vendas de ma√ß√£s na ter√ßa-feira: quantidade da segunda-feira √ó 2 = 15 √ó 2 = 30", "Calcular vendas de ma√ß√£s na quarta-feira: quantidade da ter√ßa-feira - 5 = 30 - 5 = 25", "Calcular vendas totais de tr√™s dias: segunda-feira + ter√ßa-feira + quarta-feira = 15 + 30 + 25 = 70"]
```

--- Executando Plano ---

-> Executando passo 1/4: Calcular vendas de ma√ß√£s na segunda-feira: 15
üß† Chamando o modelo xxxx...
‚úÖ Resposta do modelo de linguagem grande bem-sucedida:
15
‚úÖ Passo 1 completado, resultado: 15

-> Executando passo 2/4: Calcular vendas de ma√ß√£s na ter√ßa-feira: quantidade da segunda-feira √ó 2 = 15 √ó 2 = 30
üß† Chamando o modelo xxxx...
‚úÖ Resposta do modelo de linguagem grande bem-sucedida:
30
‚úÖ Passo 2 completado, resultado: 30

-> Executando passo 3/4: Calcular vendas de ma√ß√£s na quarta-feira: quantidade da ter√ßa-feira - 5 = 30 - 5 = 25
üß† Chamando o modelo xxxx...
‚úÖ Resposta do modelo de linguagem grande bem-sucedida:
25
‚úÖ Passo 3 completado, resultado: 25

-> Executando passo 4/4: Calcular vendas totais de tr√™s dias: segunda-feira + ter√ßa-feira + quarta-feira = 15 + 30 + 25 = 70
üß† Chamando o modelo xxxx...
‚úÖ Resposta do modelo de linguagem grande bem-sucedida:
70
‚úÖ Passo 4 completado, resultado: 70

--- Tarefa Completada ---
Resposta Final: 70
````

Do log de sa√≠da acima, podemos ver claramente o fluxo de trabalho do paradigma Plan-and-Solve:

1. **Fase de Planejamento**: O agente primeiro chama `Planner` e decomp√µe com sucesso o problema de palavra complexo em uma lista Python contendo quatro passos l√≥gicos. Este plano estruturado estabelece a base para execu√ß√£o subsequente.
2. **Fase de Execu√ß√£o**: `Executor` executa estritamente passo a passo de acordo com o plano gerado. Em cada passo, ele usa resultados hist√≥ricos como contexto, garantindo transfer√™ncia correta de informa√ß√£o (por exemplo, passo 2 usa corretamente o resultado do passo 1 "15", e passo 3 tamb√©m usa corretamente o resultado do passo 2 "30").
3. **Resultado**: Todo o processo √© logicamente claro com passos expl√≠citos, e o agente chega com precis√£o √† resposta correta "70".

## 4.4 Reflex√£o

Nos paradigmas ReAct e Plan-and-Solve que j√° implementamos, uma vez que o agente completa uma tarefa, seu fluxo de trabalho termina. No entanto, as respostas iniciais que eles geram, sejam trajet√≥rias de a√ß√£o ou resultados finais, podem conter erros ou ter espa√ßo para melhoria. A ideia central do mecanismo de Reflex√£o √© introduzir um **loop de autocorre√ß√£o p√≥s-fato** para o agente, permitindo que ele revise seu trabalho, descubra defici√™ncias e otimize iterativamente, assim como os humanos fazem.

### 4.4.1 Ideia Central do Mecanismo de Reflex√£o

A inspira√ß√£o para o mecanismo de Reflex√£o vem do processo de aprendizagem humana: revisamos ap√≥s completar um primeiro rascunho e verificamos ap√≥s resolver um problema de matem√°tica. Esta ideia est√° incorporada em m√∫ltiplos estudos, como o framework Reflexion proposto por Shinn, Noah em 2023<sup>[3]</sup>. Seu fluxo de trabalho central pode ser resumido como um loop conciso de tr√™s passos: **Executar -> Refletir -> Refinar**.

1. **Execu√ß√£o**: Primeiro, o agente tenta completar a tarefa usando m√©todos familiares (como ReAct ou Plan-and-Solve), gerando uma solu√ß√£o preliminar ou trajet√≥ria de a√ß√£o. Isso pode ser visto como um "primeiro rascunho."
2. **Reflex√£o**: Em seguida, o agente entra na fase de reflex√£o. Ele chama uma inst√¢ncia independente de modelo de linguagem grande, ou uma com prompts especiais, para desempenhar o papel de um "revisor." Este "revisor" examina o "primeiro rascunho" gerado no primeiro passo e o avalia de m√∫ltiplas dimens√µes, como:
   - **Erros Factuais**: H√° conte√∫do que contradiz senso comum ou fatos conhecidos?
   - **Falhas L√≥gicas**: H√° inconsist√™ncias ou contradi√ß√µes no processo de racioc√≠nio?
   - **Problemas de Efici√™ncia**: H√° um caminho mais direto, mais conciso para completar a tarefa?
   - **Informa√ß√£o Faltando**: Algumas restri√ß√µes-chave ou aspectos do problema s√£o negligenciados? Baseado na avalia√ß√£o, ele gera **Feedback** estruturado, apontando problemas espec√≠ficos e sugest√µes de melhoria.
3. **Refinamento**: Finalmente, o agente usa o "primeiro rascunho" e "feedback" como novo contexto, chama o modelo de linguagem grande novamente, e pede para ele revisar o primeiro rascunho baseado no conte√∫do do feedback, gerando um "rascunho revisado" mais completo.

Como mostrado na Figura 4.3, este loop pode ser repetido m√∫ltiplas vezes at√© que a fase de reflex√£o n√£o encontre mais novos problemas ou atinja um limite de itera√ß√£o predefinido. Podemos expressar formalmente este processo de otimiza√ß√£o iterativa. Assumindo que $O_i$ √© a sa√≠da produzida pela $i$-√©sima itera√ß√£o ($O_0$ √© a sa√≠da inicial), o modelo de reflex√£o $\pi_{\text{reflect}}$ gera feedback $F_i$ para $O_i$:
$$
F_i = \pi_{\text{reflect}}(\text{Tarefa}, O_i)
$$
Subsequentemente, o modelo de refinamento $\pi_{\text{refine}}$ combina a tarefa original, a sa√≠da da vers√£o anterior e feedback para gerar a sa√≠da da nova vers√£o $O_{i+1}$:
$$
O_{i+1} = \pi_{\text{refine}}(\text{Tarefa}, O_i, F_i)
$$



<div align="center">
<img src="https://raw.githubusercontent.com/datawhalechina/Hello-Agents/main/docs/images/4-figures/4-3.png" alt="Loop iterativo Executar-Refletir-Refinar no mecanismo de Reflex√£o" width="70%"/>
<p>Figura 4.3 Loop Iterativo Executar-Refletir-Refinar no Mecanismo de Reflex√£o</p>
</div>



Comparado aos dois paradigmas anteriores, o valor da Reflex√£o est√° em:

- Ela fornece ao agente um loop interno de corre√ß√£o de erros, tornando-o n√£o mais completamente dependente de feedback de ferramentas externas (Observation do ReAct), podendo assim corrigir erros l√≥gicos e estrat√©gicos de n√≠vel superior.
- Ela transforma execu√ß√£o de tarefa √∫nica em um processo de otimiza√ß√£o cont√≠nua, melhorando significativamente a taxa de sucesso final e qualidade da resposta para tarefas complexas.
- Ela constr√≥i uma **"mem√≥ria de curto prazo"** tempor√°ria para o agente. Toda a trajet√≥ria "executar-refletir-refinar" forma um registro de experi√™ncia valioso; o agente n√£o apenas sabe a resposta final mas tamb√©m lembra como iterou de um primeiro rascunho com falhas para a vers√£o final. Al√©m disso, este sistema de mem√≥ria tamb√©m pode ser **multimodal**, permitindo que o agente reflita e revise sa√≠das al√©m de texto (como c√≥digo, imagens, etc.), estabelecendo a base para construir agentes multimodais mais poderosos.

### 4.4.2 Configura√ß√£o de Caso e Design do M√≥dulo de Mem√≥ria

Para incorporar o mecanismo de Reflex√£o na pr√°tica, vamos introduzir um mecanismo de gerenciamento de mem√≥ria, porque reflex√£o geralmente corresponde a armazenamento e recupera√ß√£o de informa√ß√£o. Se o contexto for longo o suficiente, ter o "revisor" obtendo diretamente toda a informa√ß√£o e depois refletindo muitas vezes introduz muita informa√ß√£o redundante. Neste passo pr√°tico, principalmente completamos **gera√ß√£o de c√≥digo e otimiza√ß√£o iterativa**.

O objetivo da tarefa para este passo √©: "Escreva uma fun√ß√£o Python para encontrar todos os n√∫meros primos entre 1 e n."

Esta tarefa √© um cen√°rio excelente para testar o mecanismo de Reflex√£o:

1. **Caminho Claro de Otimiza√ß√£o Existe**: O c√≥digo inicialmente gerado pelo modelo de linguagem grande provavelmente ser√° uma implementa√ß√£o recursiva simples mas ineficiente.
2. **Pontos Claros de Reflex√£o**: Atrav√©s de reflex√£o, problemas como "complexidade de tempo excessivamente alta" ou "c√°lculos redundantes" podem ser descobertos.
3. **Dire√ß√£o Clara de Otimiza√ß√£o**: Baseado no feedback, pode ser otimizado para uma vers√£o iterativa mais eficiente ou uma vers√£o usando o padr√£o de memoriza√ß√£o.

O n√∫cleo da Reflex√£o est√° na itera√ß√£o, e o pr√©-requisito para itera√ß√£o √© a capacidade de lembrar tentativas anteriores e feedback recebido. Portanto, um m√≥dulo de "mem√≥ria de curto prazo" √© essencial para implementar este paradigma. Este m√≥dulo de mem√≥ria ser√° respons√°vel por armazenar a trajet√≥ria completa de cada loop "executar-refletir".

```python
from typing import List, Dict, Any, Optional

class Memory:
    """
    Um m√≥dulo de mem√≥ria de curto prazo simples para armazenar a trajet√≥ria de a√ß√£o e reflex√£o do agente.
    """

    def __init__(self):
        """
        Inicializa uma lista vazia para armazenar todos os registros.
        """
        self.records: List[Dict[str, Any]] = []

    def add_record(self, record_type: str, content: str):
        """
        Adiciona um novo registro √† mem√≥ria.

        Par√¢metros:
        - record_type (str): Tipo de registro ('execution' ou 'reflection').
        - content (str): Conte√∫do espec√≠fico do registro (por exemplo, c√≥digo gerado ou feedback de reflex√£o).
        """
        record = {"type": record_type, "content": content}
        self.records.append(record)
        print(f"üìù Mem√≥ria atualizada, adicionado um registro de '{record_type}'.")

    def get_trajectory(self) -> str:
        """
        Formata todos os registros de mem√≥ria em um texto de string coerente para construir prompts.
        """
        trajectory_parts = []
        for record in self.records:
            if record['type'] == 'execution':
                trajectory_parts.append(f"--- Tentativa Anterior (C√≥digo) ---\n{record['content']}")
            elif record['type'] == 'reflection':
                trajectory_parts.append(f"--- Feedback do Revisor ---\n{record['content']}")

        return "\n\n".join(trajectory_parts)

    def get_last_execution(self) -> Optional[str]:
        """
        Obt√©m o resultado de execu√ß√£o mais recente (por exemplo, o c√≥digo gerado mais recente).
        Retorna None se n√£o existir.
        """
        for record in reversed(self.records):
            if record['type'] == 'execution':
                return record['content']
        return None
```

O design desta classe `Memory` √© relativamente conciso, com a estrutura principal sendo:

- Usa uma lista `records` para armazenar cada a√ß√£o e reflex√£o em ordem.
- O m√©todo `add_record` √© respons√°vel por adicionar novas entradas √† mem√≥ria.
- O m√©todo `get_trajectory` √© o n√∫cleo; ele "serializa" a trajet√≥ria de mem√≥ria em um segmento de texto que pode ser diretamente inserido em prompts subsequentes, fornecendo contexto completo para reflex√£o e otimiza√ß√£o do modelo.
- `get_last_execution` torna conveniente obter o "primeiro rascunho" mais recente para reflex√£o.



### 4.4.3 Implementa√ß√£o em C√≥digo do Agente de Reflex√£o

Com o m√≥dulo `Memory` como base, podemos agora proceder para construir a l√≥gica central do `ReflectionAgent`. Todo o fluxo de trabalho do agente girar√° em torno do loop "executar-refletir-refinar" que discutimos anteriormente e guiar√° o modelo de linguagem grande a desempenhar pap√©is diferentes atrav√©s de prompts cuidadosamente projetados.

(1) Design de Prompt

Diferente dos paradigmas anteriores, o mecanismo de Reflex√£o requer m√∫ltiplos prompts para pap√©is diferentes trabalharem juntos.

1. **Prompt de Execu√ß√£o Inicial**: Este √© o prompt para a primeira tentativa do agente de resolver o problema, com conte√∫do relativamente direto, apenas exigindo que o modelo complete a tarefa especificada.

```bash
INITIAL_PROMPT_TEMPLATE = """
Voc√™ √© um programador Python s√™nior. Por favor, escreva uma fun√ß√£o Python de acordo com os seguintes requisitos.
Seu c√≥digo deve incluir assinatura de fun√ß√£o completa, docstring, e seguir padr√µes de codifica√ß√£o PEP 8.

Requisito: {task}

Por favor, produza o c√≥digo diretamente sem quaisquer explica√ß√µes adicionais.
"""
```

2. **Prompt de Reflex√£o**: Este prompt √© a alma do mecanismo de Reflex√£o. Ele instrui o modelo a desempenhar o papel de um "revisor de c√≥digo," analisar criticamente o c√≥digo gerado na rodada anterior, e fornecer feedback espec√≠fico e acion√°vel.

````bash
REFLECT_PROMPT_TEMPLATE = """
Voc√™ √© um especialista em revis√£o de c√≥digo extremamente rigoroso e engenheiro de algoritmos s√™nior com requisitos finais para desempenho de c√≥digo.
Sua tarefa √© revisar o seguinte c√≥digo Python e focar em encontrar seus principais gargalos em <strong>efici√™ncia de algoritmo</strong>.

# Tarefa Original:
{task}

# C√≥digo para Revisar:
```python
{code}
```

Por favor, analise a complexidade de tempo deste c√≥digo e considere se h√° uma solu√ß√£o <strong>algoritmicamente superior</strong> para melhorar significativamente o desempenho.
Se existir, por favor aponte claramente as defici√™ncias do algoritmo atual e proponha sugest√µes espec√≠ficas e vi√°veis de melhoria de algoritmo (por exemplo, usar m√©todo de peneira em vez de divis√£o por tentativa).
Somente se o c√≥digo tiver atingido otimalidade no n√≠vel de algoritmo voc√™ pode responder "nenhuma melhoria necess√°ria."

Por favor, produza seu feedback diretamente sem quaisquer explica√ß√µes adicionais.
"""
````

3. **Prompt de Refinamento**: Ap√≥s receber feedback, este prompt guiar√° o modelo a revisar e otimizar o c√≥digo original baseado no conte√∫do do feedback.

````bash

REFINE_PROMPT_TEMPLATE = """
Voc√™ √© um programador Python s√™nior. Voc√™ est√° otimizando seu c√≥digo baseado no feedback de um especialista em revis√£o de c√≥digo.

# Tarefa Original:
{task}

# Sua Tentativa de C√≥digo Anterior:
{last_code_attempt}
Feedback do Revisor:
{feedback}

Por favor, gere uma nova vers√£o otimizada do c√≥digo baseada no feedback do revisor.
Seu c√≥digo deve incluir assinatura de fun√ß√£o completa, docstring, e seguir padr√µes de codifica√ß√£o PEP 8.
Por favor, produza o c√≥digo otimizado diretamente sem quaisquer explica√ß√µes adicionais.
"""
````

(2) Encapsulamento e Implementa√ß√£o do Agente

Agora, vamos integrar este conjunto de l√≥gica de prompt e o m√≥dulo `Memory` na classe `ReflectionAgent`.

```python
# Assume que llm_client.py e memory.py j√° est√£o definidos
# from llm_client import HelloAgentsLLM
# from memory import Memory

class ReflectionAgent:
    def __init__(self, llm_client, max_iterations=3):
        self.llm_client = llm_client
        self.memory = Memory()
        self.max_iterations = max_iterations

    def run(self, task: str):
        print(f"\n--- Come√ßando a Processar Tarefa ---\nTarefa: {task}")

        # --- 1. Execu√ß√£o Inicial ---
        print("\n--- Realizando Tentativa Inicial ---")
        initial_prompt = INITIAL_PROMPT_TEMPLATE.format(task=task)
        initial_code = self._get_llm_response(initial_prompt)
        self.memory.add_record("execution", initial_code)

        # --- 2. Loop Iterativo: Reflex√£o e Refinamento ---
        for i in range(self.max_iterations):
            print(f"\n--- Itera√ß√£o {i+1}/{self.max_iterations} ---")

            # a. Reflex√£o
            print("\n-> Realizando Reflex√£o...")
            last_code = self.memory.get_last_execution()
            reflect_prompt = REFLECT_PROMPT_TEMPLATE.format(task=task, code=last_code)
            feedback = self._get_llm_response(reflect_prompt)
            self.memory.add_record("reflection", feedback)

            # b. Verificar se √© necess√°rio parar
            if "nenhuma melhoria necess√°ria" in feedback.lower():
                print("\n‚úÖ Reflex√£o considera que c√≥digo n√£o precisa de melhoria, tarefa completada.")
                break

            # c. Refinamento
            print("\n-> Realizando Refinamento...")
            refine_prompt = REFINE_PROMPT_TEMPLATE.format(
                task=task,
                last_code_attempt=last_code,
                feedback=feedback
            )
            refined_code = self._get_llm_response(refine_prompt)
            self.memory.add_record("execution", refined_code)

        final_code = self.memory.get_last_execution()
        print(f"\n--- Tarefa Completada ---\nC√≥digo Final Gerado:\n```python\n{final_code}\n```")
        return final_code

    def _get_llm_response(self, prompt: str) -> str:
        """Um m√©todo auxiliar para chamar LLM e obter resposta em streaming completa."""
        messages = [{"role": "user", "content": prompt}]
        response_text = self.llm_client.think(messages=messages) or ""
        return response_text

```

### 4.4.4 Inst√¢ncia de Execu√ß√£o e An√°lise

O c√≥digo completo tamb√©m pode ser encontrado na pasta `code` do reposit√≥rio de c√≥digo que acompanha este livro; aqui fornecemos uma inst√¢ncia de sa√≠da.

````python
--- Come√ßando a Processar Tarefa ---
Tarefa: Escreva uma fun√ß√£o Python para encontrar todos os n√∫meros primos entre 1 e n.

--- Realizando Tentativa Inicial ---
üß† Chamando o modelo xxxxxx...
‚úÖ Resposta do modelo de linguagem grande bem-sucedida:
```python
def find_primes(n):
    ...
    return primes
```
üìù Mem√≥ria atualizada, adicionado um registro de 'execution'.

--- Itera√ß√£o 1/2 ---

-> Realizando Reflex√£o...
üß† Chamando o modelo xxxxxx...
‚úÖ Resposta do modelo de linguagem grande bem-sucedida:
O c√≥digo atual tem complexidade de tempo de O(n * sqrt(n)). Embora esta implementa√ß√£o seja aceit√°vel para valores menores de n, o desempenho degradar√° significativamente quando n for muito grande. O principal gargalo √© que cada n√∫mero precisa de verifica√ß√£o de divis√£o por tentativa, levando a alta sobrecarga de tempo.

Recomenda-se usar o algoritmo Peneira de Erat√≥stenes, que tem complexidade de tempo de O(n log(log n)) e pode melhorar significativamente a efici√™ncia de encontrar n√∫meros primos.

C√≥digo melhorado como segue:
```python
def find_primes(n):
    ...
    return primes
```
üìù Mem√≥ria atualizada, adicionado um registro de 'reflection'.

-> Realizando Refinamento...
üß† Chamando o modelo xxxxxx...
‚úÖ Resposta do modelo de linguagem grande bem-sucedida:
```python
def find_primes(n):
    ...
    return primes
```
üìù Mem√≥ria atualizada, adicionado um registro de 'execution'.

--- Itera√ß√£o 2/2 ---

-> Realizando Reflex√£o...
üß† Chamando o modelo xxxxxx...
‚úÖ Resposta do modelo de linguagem grande bem-sucedida:
O c√≥digo atual usa a Peneira de Erat√≥stenes com complexidade de tempo O(n log log n) e complexidade de espa√ßo O(n). Este algoritmo j√° √© muito eficiente para encontrar todos os n√∫meros primos entre 1 e n, e geralmente n√£o requer otimiza√ß√£o adicional. No entanto, em alguns cen√°rios espec√≠ficos, as seguintes melhorias podem ser consideradas:

1. <strong>Peneira Segmentada</strong>: Adequada para casos onde n √© muito grande mas mem√≥ria √© limitada. Divide o intervalo em m√∫ltiplos segmentos pequenos, processa cada segmento separadamente com o m√©todo de peneira, reduzindo uso de mem√≥ria.
2. <strong>Peneira de N√∫meros √çmpares</strong>: Exceto 2, todos os n√∫meros primos s√£o √≠mpares. Ao inicializar o array `is_prime`, apenas marcar n√∫meros √≠mpares pode reduzir a complexidade de espa√ßo pela metade enquanto reduz alguns c√°lculos desnecess√°rios.

No entanto, estas melhorias n√£o s√£o necess√°rias para a maioria dos cen√°rios de aplica√ß√£o porque a Peneira de Erat√≥stenes padr√£o j√° √© eficiente o suficiente. Portanto, em casos gerais, <strong>nenhuma melhoria necess√°ria</strong>.
üìù Mem√≥ria atualizada, adicionado um registro de 'reflection'.

‚úÖ Reflex√£o considera que c√≥digo n√£o precisa de melhoria, tarefa completada.

--- Tarefa Completada ---
C√≥digo Final Gerado:
```python
def find_primes(n):
    """
    Encontra todos os n√∫meros primos entre 1 e n usando o algoritmo Peneira de Erat√≥stenes.

    :param n: O limite superior do intervalo para encontrar n√∫meros primos.
    :return: Uma lista de todos os n√∫meros primos entre 1 e n.
    """
    if n < 2:
        return []

    is_prime = [True] * (n + 1)
    is_prime[0] = is_prime[1] = False

    p = 2
    while p * p <= n:
        if is_prime[p]:
            for i in range(p * p, n + 1, p):
                is_prime[i] = False
        p += 1

    primes = [num for num in range(2, n + 1) if is_prime[num]]
    return primes
```
````

Esta inst√¢ncia de execu√ß√£o demonstra como o mecanismo de Reflex√£o impulsiona o agente a realizar otimiza√ß√£o profunda:

1. **"Cr√≠tica" Efetiva √© Pr√©-requisito para Otimiza√ß√£o**: Na primeira rodada de reflex√£o, porque usamos um prompt "extremamente rigoroso" e "focado em efici√™ncia de algoritmo," o agente n√£o ficou satisfeito com o c√≥digo inicial funcionalmente correto mas precisamente apontou seu gargalo de complexidade de tempo `O(n * sqrt(n))` e prop√¥s sugest√µes de melhoria de n√≠vel de algoritmo‚Äîa Peneira de Erat√≥stenes.
2. **Melhoria Iterativa**: Ap√≥s receber feedback claro, o agente implementou com sucesso um m√©todo de peneira mais eficiente na fase de refinamento, reduzindo a complexidade de algoritmo para `O(n log log n)`, completando a primeira auto-itera√ß√£o significativa.
3. **Converg√™ncia e T√©rmino**: Na segunda rodada de reflex√£o, enfrentando o m√©todo de peneira j√° eficiente, o agente demonstrou conhecimento mais profundo. Ele n√£o apenas afirmou a efici√™ncia do algoritmo atual mas at√© mesmo mencionou dire√ß√µes de otimiza√ß√£o mais avan√ßadas como peneira segmentada, mas finalmente fez o julgamento correto de "nenhuma melhoria necess√°ria em casos gerais." Este julgamento acionou nossa condi√ß√£o de t√©rmino, permitindo que o processo de otimiza√ß√£o convergisse.

Este caso prova completamente que o valor de um mecanismo de Reflex√£o bem projetado n√£o est√° apenas em corrigir erros mas mais importante em **impulsionar solu√ß√µes para alcan√ßar melhorias graduais em qualidade e efici√™ncia**, tornando-se uma das tecnologias-chave para construir agentes complexos e de alta qualidade.

### 4.4.5 An√°lise Custo-Benef√≠cio do Mecanismo de Reflex√£o

Embora o mecanismo de Reflex√£o tenha desempenho excelente em melhorar a qualidade de solu√ß√£o de tarefas, esta capacidade n√£o √© sem custo. Em aplica√ß√µes pr√°ticas, precisamos pesar os benef√≠cios que ele traz contra os custos correspondentes.

(1) Principais Custos

1. **Aumento da Sobrecarga de Chamadas ao Modelo**: Este √© o custo mais direto. Cada itera√ß√£o requer pelo menos duas chamadas adicionais ao modelo de linguagem grande (uma para reflex√£o, uma para refinamento). Se iterando m√∫ltiplas rodadas, custos de chamada de API e consumo de recursos computacionais aumentar√£o exponencialmente.

2. **Aumento Significativo da Lat√™ncia de Tarefa**: Reflex√£o √© um processo serial; cada rodada de refinamento deve esperar a reflex√£o da rodada anterior completar. Isso estende significativamente o tempo total da tarefa, tornando-a inadequada para cen√°rios com altos requisitos de tempo real.

3. **Aumento da Complexidade de Engenharia de Prompt**: Como nosso caso demonstra, o sucesso da Reflex√£o depende muito de prompts de alta qualidade e direcionados. Projetar e depurar prompts eficazes para diferentes fases como "execu√ß√£o," "reflex√£o," e "refinamento" requer mais esfor√ßo de desenvolvimento.

(2) Benef√≠cios Centrais

1. **Salto na Qualidade de Solu√ß√£o**: O maior benef√≠cio √© que pode otimizar iterativamente uma solu√ß√£o inicial "qualificada" em uma solu√ß√£o final "excelente." Esta melhoria de funcionalmente correto para eficiente em desempenho, de l√≥gica aproximada para l√≥gica rigorosa, √© crucial em muitas tarefas cr√≠ticas.

2. **Robustez e Confiabilidade Melhoradas**: Atrav√©s de loops internos de autocorre√ß√£o, o agente pode descobrir e corrigir falhas l√≥gicas potenciais, erros factuais ou manipula√ß√£o inadequada de casos de borda na solu√ß√£o inicial, melhorando muito a confiabilidade do resultado final.

Em resumo, o mecanismo de Reflex√£o √© uma estrat√©gia t√≠pica de "custo por qualidade." √â muito adequado para cen√°rios que **t√™m requisitos extremamente altos para qualidade, precis√£o e confiabilidade dos resultados finais, e t√™m requisitos relativamente relaxados para tempo real de conclus√£o de tarefa**. Por exemplo:

- Gerar c√≥digo de neg√≥cio cr√≠tico ou relat√≥rios t√©cnicos.
- Conduzir racioc√≠nio l√≥gico complexo em pesquisa cient√≠fica.
- Sistemas de suporte a decis√£o requerendo an√°lise e planejamento profundos.

Por outro lado, se o cen√°rio de aplica√ß√£o requer respostas r√°pidas, ou uma resposta "aproximadamente correta" j√° √© suficiente, usar paradigmas ReAct ou Plan-and-Solve mais leves pode ser uma escolha mais custo-efetiva.

## 4.5 Resumo do Cap√≠tulo

Neste cap√≠tulo, baseando-nos no conhecimento de modelos de linguagem grandes dominado no Cap√≠tulo 3, codificamos e implementamos tr√™s paradigmas cl√°ssicos de constru√ß√£o de agentes da ind√∫stria do zero atrav√©s de "construir rodas n√≥s mesmos": ReAct, Plan-and-Solve e Reflex√£o. N√£o apenas exploramos seus princ√≠pios de funcionamento centrais mas tamb√©m compreendemos profundamente suas respectivas vantagens, limita√ß√µes e cen√°rios aplic√°veis atrav√©s de casos pr√°ticos espec√≠ficos.

**Revis√£o de Conhecimento Central:**

1. ReAct: Constru√≠mos um agente ReAct que pode interagir com o mundo externo. Atrav√©s do loop din√¢mico de "pensamento-a√ß√£o-observa√ß√£o," ele usou com sucesso mecanismos de busca para responder perguntas em tempo real que sua pr√≥pria base de conhecimento n√£o poderia cobrir. Suas vantagens centrais est√£o em **adaptabilidade ambiental** e **capacidade de corre√ß√£o de erros din√¢mica**, tornando-o a primeira escolha para lidar com tarefas explorat√≥rias requerendo entrada de ferramentas externas.
2. Plan-and-Solve: Implementamos um agente Plan-and-Solve que planeja primeiro depois executa, e o usamos para resolver problemas de palavras matem√°ticos requerendo racioc√≠nio de m√∫ltiplos passos. Ele decomp√µe tarefas complexas em passos claros, depois os executa um por um. Suas vantagens centrais est√£o em **estrutura** e **estabilidade**, particularmente adequadas para lidar com tarefas com caminhos l√≥gicos determinados e racioc√≠nio interno intensivo.
3. Reflex√£o (Auto-Reflex√£o e Itera√ß√£o): Constru√≠mos um agente de Reflex√£o com capacidades de auto-otimiza√ß√£o. Ao introduzir o loop iterativo "executar-refletir-refinar," ele otimizou com sucesso uma solu√ß√£o de c√≥digo inicialmente ineficiente em uma vers√£o de alto desempenho algoritmicamente superior. Seu valor central est√° em **melhorar significativamente a qualidade de solu√ß√£o**, adequado para cen√°rios com requisitos extremamente altos para precis√£o e confiabilidade de resultados.

Os tr√™s paradigmas explorados neste cap√≠tulo representam tr√™s estrat√©gias diferentes para agentes resolverem problemas, como mostrado na Tabela 4.1. Em aplica√ß√µes pr√°ticas, qual escolher depende dos requisitos centrais da tarefa:

<div align="center">
<p>Tabela 4.1 Estrat√©gia de Sele√ß√£o para Diferentes Loops de Agente</p>
<img src="https://raw.githubusercontent.com/datawhalechina/Hello-Agents/main/docs/images/4-figures/4-4.png" alt="" width="70%"/>
</div>

Neste ponto, dominamos as tecnologias centrais para construir agentes individuais. Para transitar conhecimento e obter insights mais profundos sobre aplica√ß√µes pr√°ticas, na pr√≥xima se√ß√£o vamos explorar como usar diferentes plataformas low-code e solu√ß√µes leves de c√≥digo para construir agentes.

## Exerc√≠cios

> **Nota**: Alguns exerc√≠cios n√£o t√™m respostas padr√£o; o foco est√° em cultivar a compreens√£o abrangente e capacidade pr√°tica dos aprendizes em design de paradigma de agente.

1. Este cap√≠tulo introduziu tr√™s paradigmas cl√°ssicos de agente: `ReAct`, `Plan-and-Solve`, e `Reflection`. Por favor analise:

   - Quais s√£o as diferen√ßas essenciais em como esses tr√™s paradigmas organizam "pensamento" e "a√ß√£o"?
   - Se voc√™ fosse projetar um "assistente de controle de casa inteligente" (precisa controlar luzes, ar condicionado, cortinas e outros dispositivos, e ajustar automaticamente baseado em h√°bitos do usu√°rio), qual paradigma voc√™ escolheria como arquitetura b√°sica? Por qu√™?
   - Esses tr√™s paradigmas podem ser combinados? Se sim, por favor tente projetar uma arquitetura de agente de paradigma h√≠brido e explique seus cen√°rios aplic√°veis.

2. Na implementa√ß√£o `ReAct` na Se√ß√£o 4.2, usamos express√µes regulares para analisar a sa√≠da do modelo de linguagem grande (como `Thought` e `Action`). Por favor considere:

   - Que fragilidades potenciais existem no m√©todo de an√°lise atual? Sob que circunst√¢ncias pode falhar?
   - Al√©m de express√µes regulares, quais s√£o algumas solu√ß√µes de an√°lise de sa√≠da mais robustas?
   - Tente modificar o c√≥digo neste cap√≠tulo para usar um formato de sa√≠da mais confi√°vel, e compare os pr√≥s e contras das duas abordagens.

3. Invoca√ß√£o de ferramentas √© uma das capacidades centrais de agentes modernos. Baseado no design `ToolExecutor` na Se√ß√£o 4.2.2, por favor complete a seguinte pr√°tica de extens√£o:

   > **Nota**: Esta √© uma quest√£o de pr√°tica hands-on; √© recomendado escrever c√≥digo de fato.

   - Adicione uma ferramenta "calculadora" ao agente `ReAct` para que ele possa lidar com problemas de c√°lculos matem√°ticos complexos (como "Calcule o resultado de `(123 + 456) √ó 789 / 12 = ?`").
   - Projete e implemente um mecanismo de tratamento de "falha de sele√ß√£o de ferramenta": quando o agente chama repetidamente a ferramenta errada ou fornece par√¢metros errados, como o sistema deve gui√°-lo a corrigir?
   - Considere: Se o n√∫mero de ferramentas cham√°veis aumentar para 50 ou at√© 100, o m√©todo de descri√ß√£o de ferramenta atual ainda funcionar√° efetivamente? De uma perspectiva de engenharia, como podemos otimizar a organiza√ß√£o e mecanismo de recupera√ß√£o de ferramentas quando o n√∫mero de ferramentas cham√°veis aumenta significativamente com necessidades de neg√≥cio?

4. O paradigma `Plan-and-Solve` decomp√µe tarefas em duas fases: "planejamento" e "execu√ß√£o." Por favor analise em profundidade:

   - Na implementa√ß√£o na Se√ß√£o 4.3, o plano gerado na fase de planejamento √© "est√°tico" (gerado uma vez, n√£o modific√°vel). Se durante a execu√ß√£o for descoberto que um certo passo n√£o pode ser completado ou o resultado n√£o atende √†s expectativas, como um mecanismo de "replanejamento din√¢mico" deveria ser projetado?
   - Compare `Plan-and-Solve` com `ReAct`: Ao lidar com uma tarefa como "reservar uma viagem de neg√≥cios de Beijing a Shanghai (incluindo voos, hot√©is, aluguel de carro)," qual paradigma √© mais adequado? Por qu√™?
   - Tente projetar um sistema de "planejamento hier√°rquico": primeiro gere um plano abstrato de alto n√≠vel, depois gere sub-planos detalhados para cada passo de alto n√≠vel. Quais vantagens este design tem?

5. O mecanismo `Reflection` melhora qualidade de sa√≠da atrav√©s do loop "executar-refletir-refinar." Por favor considere:

   - No caso de gera√ß√£o de c√≥digo na Se√ß√£o 4.4, o mesmo modelo √© usado para diferentes fases. Se dois modelos diferentes fossem usados (por exemplo, usar um modelo mais poderoso para reflex√£o e um modelo mais r√°pido para execu√ß√£o), que impacto teria?
   - A condi√ß√£o de t√©rmino para o mecanismo `Reflection` √© "feedback cont√©m **nenhuma melhoria necess√°ria**" ou "contagem m√°xima de itera√ß√£o atingida." Este design √© razo√°vel? Uma condi√ß√£o de t√©rmino mais inteligente pode ser projetada?
   - Suponha que voc√™ queira construir um "assistente de escrita de artigos acad√™micos" que pode gerar rascunhos e continuamente otimizar conte√∫do de artigo. Por favor projete um mecanismo de Reflex√£o multidimensional que reflete e melhora de m√∫ltiplas perspectivas como l√≥gica de par√°grafo, inova√ß√£o de m√©todo, express√£o de linguagem e padr√µes de cita√ß√£o.

6. Engenharia de prompt √© uma tecnologia-chave afetando o efeito final de agentes. Este cap√≠tulo demonstrou m√∫ltiplos modelos de prompt cuidadosamente projetados. Por favor analise:

   - Compare o prompt `ReAct` na Se√ß√£o 4.2.3 e o prompt `Plan-and-Solve` na Se√ß√£o 4.3.2; eles obviamente t√™m diferen√ßas significativas em design estrutural. Como essas diferen√ßas servem √† l√≥gica central de seus respectivos paradigmas?
   - No prompt `Reflection` na Se√ß√£o 4.4.3, usamos uma configura√ß√£o de papel como "voc√™ √© um especialista em revis√£o de c√≥digo extremamente rigoroso." Tente modificar esta configura√ß√£o de papel (como mud√°-la para "voc√™ √© um mantenedor de projeto open-source que valoriza legibilidade de c√≥digo"), observe as mudan√ßas nos resultados de sa√≠da, e resuma o impacto de configura√ß√µes de papel no comportamento do agente.
   - Adicionar exemplos `few-shot` aos prompts muitas vezes pode melhorar significativamente a capacidade do modelo de seguir formatos espec√≠ficos. Por favor tente adicionar exemplos `few-shot` a um dos agentes neste cap√≠tulo e compare os efeitos.

7. Uma startup de e-commerce agora espera usar um "agente de atendimento ao cliente" para substituir atendimento ao cliente humano para redu√ß√£o de custos e melhoria de efici√™ncia. Ele precisa ter as seguintes fun√ß√µes:

   a. Entender a raz√£o da solicita√ß√£o de reembolso do usu√°rio

   b. Consultar informa√ß√µes de pedido do usu√°rio e status de log√≠stica

   c. Julgar inteligentemente se o reembolso deve ser aprovado baseado na pol√≠tica da empresa

   d. Gerar um e-mail de resposta apropriado e envi√°-lo ao e-mail do usu√°rio

   e. Se a decis√£o de julgamento √© de alguma forma controversa (autoconfian√ßa est√° abaixo de um limiar), ser capaz de auto-refletir e fornecer sugest√µes mais prudentes

   Como gerente de produto deste produto:
   - Qual paradigma (ou combina√ß√£o de paradigmas) deste cap√≠tulo voc√™ escolheria como arquitetura central do sistema?
   - Que ferramentas este sistema precisa? Por favor liste pelo menos 3 ferramentas e suas descri√ß√µes funcionais.
   - Como projetar prompts para garantir que as decis√µes do agente tanto se alinhem com interesses da empresa quanto mantenham uma atitude amig√°vel para com usu√°rios?
   - Que riscos e desafios este produto pode enfrentar ap√≥s lan√ßamento? Como esses riscos podem ser reduzidos atrav√©s de meios t√©cnicos?

## Refer√™ncias

[1] Yao S, Zhao J, Yu D, et al. React: Synergizing reasoning and acting in language models[C]//International Conference on Learning Representations (ICLR). 2023.

[2] Wang L, Xu W, Lan Y, et al. Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models[J]. arXiv preprint arXiv:2305.04091, 2023.

[3] Shinn N, Cassano F, Gopinath A, et al. Reflexion: Language agents with verbal reinforcement learning[J]. Advances in Neural Information Processing Systems, 2023, 36: 8634-8652.
